---
title: "p8105_hw5_jz3183"
author: "Jungang Zou"
date: "11/2/2019"
output: github_document
---

```{r set up, include = FALSE}
# knitr will run the chunk but not include the chunk in the final document
# copy from Jeff

# ensure reproductivity
set.seed(10)

# load library
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)
library(rvest)

knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

# Homework 5 for Data Science Course p8105

## Problem 1

First, we read the data.

```{r p1_read_data, collapse = TRUE}
# In this code chunk, we will read the dataset iris
iris_with_missing = 
  iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

# display
iris_with_missing
```

Then, we need to write 1 function to fill the missing values according to the following rules:

* For numeric variables, you should fill in missing values with the mean of non-missing values
* For character variables, you should fill in missing values with "virginica"

In the next code chunk, I write a function to fill the missing value and display the data.
```{r p1_functions, collapse = TRUE}
# in this code chunk, we write a function "fill_missing" and use "map_df" to fill the missing data. 
fill_missing = function(vec) {
  if (!is.vector(vec)) {
    stop("The input is not a vector")
  }
  if (is.numeric(vec)) {
    vec = 
      vec %>% 
      replace_na(mean(vec[!is.na(vec)]))
  } else if (is.character(vec)) {
    vec = 
      vec %>% 
      replace_na("virginica")
  }
  vec
}

# fill the missing data
iris_without_missing <- map_df(.x = iris_with_missing, ~ fill_missing(.x))

# display
iris_without_missing
```


## Problem 2

In this problem, we need to firstly read datasets from local files according to the process:

* Start with a dataframe containing all file names; the list.files function will help
* Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
* Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary

```{r p2_read_data, collapse = TRUE, message = FALSE}
# In this code chunk, we will read the datasets

# read the file names for all datasets
dataset_name = 
  list.files("./dataset_p2/data", pattern = "^.+_.+.csv$") %>% 
  as.data.frame() %>% 
  map_dfc(.x = ., ~paste("./dataset_p2/data", ., sep = "/")) %>%              # add path
  rename("file_name" = ".")

# display
dataset_name


# read dataset as a dataframe and create a variable "file_data" to save the data
nest_data = 
  dataset_name %>% 
  # create a variable named file_data to denote the nested data
  mutate("file_data" = purrr::map(.x = pull(., file_name), read_csv))        # read data
  
# display
nest_data  

# tidy the nest_data
df_p2 = 
  nest_data %>% 
  mutate(file_name = str_extract(file_name, "[conexp]{3}_[0-9]{2}")) %>%     # delete ".csv" in file names
  separate(file_name, into = c("arm", "subject_id"), sep = "_") %>%          # seperate filename into arm and id
  mutate(arm = str_replace(arm, "con", "control"), arm = str_replace(arm, "exp", "experimental")) %>% 
  #mutate(subject_id = as.numeric(subject_id)) %>%                            # convert id into numeric
  unnest(cols = "file_data")                                                  # unnest
  

# display
df_p2
```

After tidy the data, we will make a spaghetti plot showing observations on each subject over time:

```{r p2_spaghetti}
# in this code chunk, we will make a spaghetti plot

df_p2 %>% 
  # transform the data to make plot
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "value"
  ) %>% 
  # draw spaghetti plot
  ggplot(aes(x = week, y = value, color = subject_id)) +
  geom_line(aes(x = week, y = value, group = subject_id)) +
  facet_grid(.~arm) +
  labs(
    title = "Spaghetti Plot for Arms in Differet Weeks",
    x = "Week",
    y = "Value",
    caption = "Data from URL: https://p8105.com/data/hw5_data.zip") +
  theme(plot.title = element_text(hjust = 0.5))
```

From the spaghetti graph, we may draw the conclusion that the values of control arm are nearly kept during 8 weeks. However, the values of experimental arm are increased during 8 weeks. This result may show that the experiment can make a difference during the time.


## Problem 3

In the problem, we are asked to do simulation on linear regression with the following parameters:

* Fix $n = 30$
* Fix $x_{i1}$ as draws from a standard Normal distribution
* Fix $\beta_0 = 2$
* Fix $\sigma^2 = 50$

During the simulations, we will set $\beta_{1} = 0$ firstly, and change its value from 1 to 6 to do the simulations.

```{r p3_beta_0, collapse = TRUE, message = FALSE}
# in this code chunk I run the simulation 10000 times for each beta_1

# simulation function
sim_regression = function(beta_1 = 0) {
  # set parameters 
  n = 30
  beta_0 = 2
  sigma = 50
  
  # set variables
  sim_data = tibble(
    x = rnorm(n, mean = 0, sd = 1),
    y = beta_0 + beta_1 * x + rnorm(n, 0, sqrt(sigma))
  )
  
  # regression
  ls_fit = lm(y ~ x, data = sim_data)
  lm_result = broom::tidy(ls_fit)
  
  # return the results of beta1_hat
  tibble(
    beta1_hat = lm_result[[2, "estimate"]],
    p_value = lm_result[[2, "p.value"]]
  )
}

sim_results = 
  tibble(beta_1 = 0:6) %>% 
  mutate(beta_1_hat = map(.x = beta_1, ~rerun(10000, sim_regression(beta_1 = .x)))) %>%  #simulate
  # unnest
  unnest(cols = "beta_1_hat") %>%   
  unnest(cols = "beta_1_hat")

# display
head(sim_results)
```

After simulation, we will start to draw graph to make some inference. The first is to make a plot showing the proportion of times the null was rejected (the power of the test):

```{r p3_power, collapse = TRUE, message = FALSE}
# in this code chunk, we make a plot about power

sim_results %>% 
  # calculate reject_proportion
  mutate(reject = if_else(p_value <= 0.05, TRUE, FALSE)) %>% 
  group_by(beta_1, reject) %>% 
  summarise(reject_proportion = n() / 10000) %>% 
  filter(reject == TRUE) %>% 
  
  # make bar plot
  ggplot(aes(x = beta_1, y = reject_proportion, fill = beta_1)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6)
  ) + 
  scale_y_continuous(
    breaks = c(0, 0.25, 0.5, 0.75, 1),
    labels = c("0%", "25%", "50%", "75%", "100%")
  ) + 
  labs(
    title = "Bar Plot of the Proportion of Times the Null was Rejected",
    x = "Beta_1",
    y = "Rejection Proportion",
    caption = "Data from Jeff"
    ) +
  theme(plot.title = element_text(hjust = 0.5))
```

From the par plot we find with the effect size increase, the power is also increasing.

Then, I will make 2 plots showing the average estimate of $\hat{\beta_1}$:

```{r p3_average, collapse = TRUE, message = FALSE}
# in this code chunk, we make a line plot of beta_1 average 

sim_results %>% 
  # calculate rejection
  mutate(reject = if_else(p_value <= 0.05, TRUE, FALSE)) %>% 
  
  # calculate average on all data
  group_by(beta_1) %>%
  mutate(average_all_beta1_hat = mean(beta1_hat)) %>% 
  ungroup() %>% 
  
  # calculate average on rejected data
  group_by(beta_1, reject) %>% 
  mutate(average_reject_beta1_hat = mean(beta1_hat)) %>% 
  filter(reject) %>% 
  ungroup() %>% 
  
  # data transformation
  select(beta_1, average_reject_beta1_hat, average_all_beta1_hat) %>% 
  distinct() %>% 
  pivot_longer(
    cols = c(average_reject_beta1_hat, average_all_beta1_hat),
    names_to = "average_beta1_hat"
  ) %>% 
  
  # make line plot
  ggplot(aes(x = beta_1, y = value, color = average_beta1_hat)) +
  geom_point(size = 5) +
  geom_line(aes(group = average_beta1_hat)) +
  scale_color_discrete("Samples",labels = c("All data","Rejected data")) +
  scale_x_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6)
  ) + 
  scale_y_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6, 10, 15, 20)
  ) + 
  labs(
    title = "Line Plot of the Average Beta1_hat for Different Samples",
    x = "Beta_1",
    y = "Average Beta1_hat",
    caption = "Data from Jeff") +
  theme(plot.title = element_text(hjust = 0.5))
```

From the test we do, we find the sample average of $\hat{\beta_1}$ across tests for which the null is rejected gradually equal to the true value of $\beta_1$ with the increasing value of $\beta_1$. On the other hand, sample average of $\hat{\beta_1}$ on all data is always consistent to the true value of $\beta_1$. The reason is the samples of $\hat{\beta_1}$ which can across tests for which the null is rejected are more likely to be deviated from $\beta_1 = 0$ when $\beta_1 = 0$ is close to 0. However, with the increasing value of $\beta_1 = 0$ , more and more samples rejected the null hypothesis, which makes the average of $\hat{\beta_1}$ on rejected data closer to the average of  $\hat{\beta_1}$ on all data.